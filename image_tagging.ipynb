{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\abed\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\abed\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abed\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abed\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abed\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abed\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in c:\\users\\abed\\appdata\\roaming\\python\\python312\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\abed\\appdata\\roaming\\python\\python312\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras._tf_keras.keras.preprocessing import image\n",
    "from keras._tf_keras.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras._tf_keras.keras.applications import MobileNetV2, EfficientNetB0\n",
    "# Other to try or combine: ResNet50, Xception, VGG16, InceptionV3\n",
    "from keras._tf_keras.keras.models import Model, Sequential, load_model\n",
    "from keras._tf_keras.keras.utils import load_img, img_to_array, to_categorical \n",
    "from keras._tf_keras.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras._tf_keras.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = \"source_directory/WARDE_WEB.csv\"\n",
    "destination_file = \"source_directory/Image_Tagging_Dataset.csv\"\n",
    "image_folder = \"source_directory/Warde\"\n",
    "new_image_folder = \"source_directory/Images\"\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"TYPE\": [\n",
    "        \"velvet\", \"curtain\", \"double purpose\", \"upholstery\", \"wallcovering\",\n",
    "        \"embroidery\", \"faux fur\", \"faux leather\", \"jacquard\", \"microfiber\",\n",
    "        \"organic\", \"print & embossed\", \"satin\", \"sheer\", \"suede\", \"sunscreen\",\n",
    "        \"wallpanel\", \"wallpaper\", \"weave\"\n",
    "    ],\n",
    "    \"COLOR\": [\n",
    "        \"black\", \"blue\", \"brown\", \"dark beige\", \"dark grey\", \"green\", \"light beige\",\n",
    "        \"light grey\", \"metallic\", \"multicolor\", \"orange\", \"pink\", \"purple\", \"red\",\n",
    "        \"white\", \"yellow\"\n",
    "    ],\n",
    "    \"STYLE\": [\n",
    "        \"children\", \"classical\", \"contemporary & modern\", \"ethnic & oriental\", \"floral\",\n",
    "        \"geometric\", \"illustrative\", \"stripes; checks; and zigzags\", \"plain\", \"textured\"\n",
    "    ],\n",
    "    \"USAGE\": [\n",
    "        \"curtain\", \"double purpose\", \"upholstery\", \"wallcovering\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset and Images saved to source_directory/Image_Tagging_Dataset.csv and source_directory/Images\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(new_image_folder) or os.path.exists(destination_file):\n",
    "    print(\"The folder or CSV file already exists.\")\n",
    "    raise SystemExit()\n",
    "\n",
    "os.makedirs(new_image_folder, exist_ok=True)\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"Item Code\", \"Usage\", \"TYPE 1\", \"TYPE 2\", \"TYPE 3\", \n",
    "    \"Color1\", \"Color2\", \"Color3\", \"Style 1\", \"Style 2\", \"Style 3\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(source_file)\n",
    "df['Item Code'] = df['Item Code'].apply(lambda x: (x.strip().replace(\" \", \"\") + \".jpg\"))\n",
    "filtered_df = df[columns_to_keep]\n",
    "columns_to_check = [\"TYPE 2\", \"TYPE 3\", \"Color2\", \"Color3\", \"Style 2\", \"Style 3\"]\n",
    "filtered_df[columns_to_check] = filtered_df[columns_to_check].replace(\"0\", np.nan)\n",
    "filtered_df.loc[:, filtered_df.columns != \"Item Code\"] = (\n",
    "    filtered_df.loc[:, filtered_df.columns != \"Item Code\"]\n",
    "    .applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    ")\n",
    "\n",
    "def image_exists(item_code):\n",
    "\n",
    "    # if item_code == \"MNM6009.jpg\":\n",
    "    #     return False\n",
    "    # Not nessescarly this file, but \n",
    "    # Copying large images sometimes \n",
    "    # cause errors\n",
    "\n",
    "    image_file = os.path.join(image_folder, f\"{item_code}\")\n",
    "    image_file = image_file.replace(\"\\\\\", \"/\")\n",
    "    if os.path.exists(image_file):\n",
    "        destination_image_path = os.path.join(new_image_folder, os.path.basename(image_file))\n",
    "        shutil.copy(image_file, destination_image_path)\n",
    "    return os.path.exists(image_file)\n",
    "\n",
    "filtered_df = filtered_df[filtered_df[\"Item Code\"].apply(image_exists)]\n",
    "filtered_df.to_csv(destination_file, index=False)\n",
    "\n",
    "print(f\"Filtered dataset and Images saved to {destination_file} and {new_image_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "data = pd.read_csv(destination_file)\n",
    "\n",
    "def binary_encode(df, column_prefix, categories):\n",
    "    encoded = pd.DataFrame(0, index=df.index, columns=categories)\n",
    "    for col in [col for col in df.columns if column_prefix in col]:\n",
    "        for category in categories:\n",
    "            encoded[category] |= (df[col] == category).astype(int)\n",
    "    return encoded\n",
    "\n",
    "type_encoded = binary_encode(data, \"TYPE\", CATEGORIES[\"TYPE\"])\n",
    "color_encoded = binary_encode(data, \"Color\", CATEGORIES[\"COLOR\"])\n",
    "style_encoded = binary_encode(data, \"Style\", CATEGORIES[\"STYLE\"])\n",
    "usage_encoded = pd.get_dummies(data[\"Usage\"], prefix=\"Usage\")\n",
    "usage_encoded = usage_encoded.reindex(\n",
    "    columns=[f\"Usage_{cat}\" for cat in CATEGORIES[\"USAGE\"]],\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "encoded_labels = pd.concat([usage_encoded, type_encoded, color_encoded, style_encoded, ], axis=1)\n",
    "encoded_labels = encoded_labels.astype(\"float32\")\n",
    "\n",
    "image_directory = \"source_directory/Images\"  \n",
    "data[\"source_directory/Images\"] = data[\"Item Code\"].apply(lambda x: os.path.join(image_directory, x))\n",
    "data = data[data[\"source_directory/Images\"].apply(os.path.exists)]\n",
    "\n",
    "def load_images(file_paths, target_size=(224, 224)):\n",
    "    images = []\n",
    "    for destination_file in file_paths:\n",
    "        img = tf.keras.utils.load_img(destination_file, target_size=target_size)\n",
    "        img = tf.keras.utils.img_to_array(img) / 255.0  \n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "images = load_images(data[\"source_directory/Images\"])\n",
    "labels = encoded_labels.loc[data.index].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USAGE (expected 4): 4\n",
      "TYPE (expected 20): 19\n",
      "COLOR (expected 17): 16\n",
      "STYLE (expected 11): 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"USAGE (expected 4): {usage_encoded.shape[1]}\")\n",
    "print(f\"TYPE (expected 20): {type_encoded.shape[1]}\")\n",
    "print(f\"COLOR (expected 17): {color_encoded.shape[1]}\")\n",
    "print(f\"STYLE (expected 11): {style_encoded.shape[1]}\")\n",
    "\n",
    "# USAGE (expected 4): 4\n",
    "# TYPE (expected 20): 19\n",
    "# COLOR (expected 17): 16\n",
    "# STYLE (expected 11): 10\n",
    "\n",
    "# Values check after encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 9s/step - loss: 1.1843 - type_color_style_accuracy: 0.2177 - type_color_style_loss: 0.5709 - usage_accuracy: 0.8416 - usage_loss: 0.6123 - val_loss: 0.2724 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.2709 - val_usage_accuracy: 1.0000 - val_usage_loss: 6.1467e-08\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 9s/step - loss: 0.2899 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2898 - usage_accuracy: 1.0000 - usage_loss: 1.1900e-07 - val_loss: 0.2083 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.2062 - val_usage_accuracy: 1.0000 - val_usage_loss: 8.6914e-06\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 10s/step - loss: 0.2326 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2325 - usage_accuracy: 1.0000 - usage_loss: 1.7150e-05 - val_loss: 0.2047 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.2026 - val_usage_accuracy: 1.0000 - val_usage_loss: 1.5555e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 6s/step - loss: 0.2151 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2150 - usage_accuracy: 1.0000 - usage_loss: 9.2610e-05 - val_loss: 0.2026 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.2002 - val_usage_accuracy: 1.0000 - val_usage_loss: 2.0063e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 7s/step - loss: 0.2128 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2127 - usage_accuracy: 1.0000 - usage_loss: 1.0439e-04 - val_loss: 0.2026 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.2001 - val_usage_accuracy: 1.0000 - val_usage_loss: 3.1837e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 6s/step - loss: 0.2140 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2138 - usage_accuracy: 1.0000 - usage_loss: 1.8026e-04 - val_loss: 0.2020 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1996 - val_usage_accuracy: 1.0000 - val_usage_loss: 3.1977e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 6s/step - loss: 0.2103 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2101 - usage_accuracy: 1.0000 - usage_loss: 1.6902e-04 - val_loss: 0.2013 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1988 - val_usage_accuracy: 1.0000 - val_usage_loss: 4.0316e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 7s/step - loss: 0.2097 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2095 - usage_accuracy: 1.0000 - usage_loss: 2.1000e-04 - val_loss: 0.2010 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1985 - val_usage_accuracy: 1.0000 - val_usage_loss: 4.6716e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 7s/step - loss: 0.2079 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2076 - usage_accuracy: 1.0000 - usage_loss: 2.4329e-04 - val_loss: 0.2002 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1978 - val_usage_accuracy: 1.0000 - val_usage_loss: 4.1191e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 10s/step - loss: 0.2068 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2066 - usage_accuracy: 1.0000 - usage_loss: 2.7447e-04 - val_loss: 0.1999 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1977 - val_usage_accuracy: 1.0000 - val_usage_loss: 3.4391e-04\n"
     ]
    }
   ],
   "source": [
    "# Without Pretrained Model\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "usage_train = y_train[:, :4]\n",
    "type_train = y_train[:, 4:24]\n",
    "color_train = y_train[:, 24:41]\n",
    "style_train = y_train[:, 41:]\n",
    "\n",
    "usage_val = y_val[:, :4]\n",
    "type_val = y_val[:, 4:24]\n",
    "color_val = y_val[:, 24:41]\n",
    "style_val = y_val[:, 41:]\n",
    "\n",
    "usage_test = y_test[:, :4]\n",
    "type_test = y_test[:, 4:24]\n",
    "color_test = y_test[:, 24:41]\n",
    "style_test = y_test[:, 41:]\n",
    "\n",
    "y_train_type_color_style = np.concatenate([type_train, color_train, style_train], axis=1)\n",
    "y_val_type_color_style = np.concatenate([type_val, color_val, style_val], axis=1)\n",
    "y_test_type_color_style = np.concatenate([type_test, color_test, style_test], axis=1)\n",
    "\n",
    "def custom_data_gen(x, y_type_color_style, y_usage, batch_size):\n",
    "    num_samples = x.shape[0]\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            x_batch = x[offset:offset+batch_size]\n",
    "            y_type_color_style_batch = y_type_color_style[offset:offset+batch_size]\n",
    "            y_usage_batch = y_usage[offset:offset+batch_size]\n",
    "            yield x_batch, {'type_color_style': y_type_color_style_batch, 'usage': y_usage_batch}\n",
    "\n",
    "batch_size = 32\n",
    "train_data_gen = custom_data_gen(X_train, y_train_type_color_style, usage_train, batch_size)\n",
    "val_data_gen = custom_data_gen(X_val, y_val_type_color_style, usage_val, batch_size)\n",
    "\n",
    "input_layer = Input(shape=(224, 224, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "type_color_style_output = Dense(y_train_type_color_style.shape[1], activation='sigmoid', name='type_color_style')(x)\n",
    "usage_output = Dense(4, activation='softmax', name='usage')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[type_color_style_output, usage_output])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'type_color_style': 'binary_crossentropy', 'usage': 'categorical_crossentropy'},\n",
    "    metrics={'type_color_style': 'accuracy', 'usage': 'accuracy'}\n",
    ")\n",
    "\n",
    "train_steps_per_epoch = np.ceil(len(X_train) / batch_size).astype(int)\n",
    "val_steps_per_epoch = np.ceil(len(X_val) / batch_size).astype(int)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "model.save(\"multi_label_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 4s/step - loss: 1.1449 - type_color_style_accuracy: 0.3589 - type_color_style_loss: 0.5809 - usage_accuracy: 0.7973 - usage_loss: 0.5631 - val_loss: 0.2078 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.2050 - val_usage_accuracy: 1.0000 - val_usage_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - loss: 0.2230 - type_color_style_accuracy: 0.9713 - type_color_style_loss: 0.2230 - usage_accuracy: 1.0000 - usage_loss: 0.0000e+00 - val_loss: 0.1635 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1613 - val_usage_accuracy: 1.0000 - val_usage_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - loss: 0.1903 - type_color_style_accuracy: 0.9050 - type_color_style_loss: 0.1902 - usage_accuracy: 1.0000 - usage_loss: 0.0000e+00 - val_loss: 0.1543 - val_type_color_style_accuracy: 0.9259 - val_type_color_style_loss: 0.1537 - val_usage_accuracy: 1.0000 - val_usage_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - loss: 0.1572 - type_color_style_accuracy: 0.9039 - type_color_style_loss: 0.1571 - usage_accuracy: 1.0000 - usage_loss: 5.6991e-10 - val_loss: 0.1444 - val_type_color_style_accuracy: 0.9352 - val_type_color_style_loss: 0.1443 - val_usage_accuracy: 1.0000 - val_usage_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3s/step - loss: 0.1464 - type_color_style_accuracy: 0.8939 - type_color_style_loss: 0.1463 - usage_accuracy: 1.0000 - usage_loss: 8.7501e-09 - val_loss: 0.1389 - val_type_color_style_accuracy: 0.9352 - val_type_color_style_loss: 0.1392 - val_usage_accuracy: 1.0000 - val_usage_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 3s/step - loss: 0.1291 - type_color_style_accuracy: 0.9021 - type_color_style_loss: 0.1291 - usage_accuracy: 1.0000 - usage_loss: 6.8089e-09 - val_loss: 0.1373 - val_type_color_style_accuracy: 0.9444 - val_type_color_style_loss: 0.1372 - val_usage_accuracy: 1.0000 - val_usage_loss: 9.3132e-10\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - loss: 0.1222 - type_color_style_accuracy: 0.9313 - type_color_style_loss: 0.1222 - usage_accuracy: 1.0000 - usage_loss: 6.7644e-09 - val_loss: 0.1333 - val_type_color_style_accuracy: 0.9352 - val_type_color_style_loss: 0.1336 - val_usage_accuracy: 1.0000 - val_usage_loss: 9.3132e-10\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3s/step - loss: 0.1135 - type_color_style_accuracy: 0.9122 - type_color_style_loss: 0.1135 - usage_accuracy: 1.0000 - usage_loss: 1.8510e-08 - val_loss: 0.1304 - val_type_color_style_accuracy: 0.9167 - val_type_color_style_loss: 0.1305 - val_usage_accuracy: 1.0000 - val_usage_loss: 1.8626e-09\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - loss: 0.1058 - type_color_style_accuracy: 0.9054 - type_color_style_loss: 0.1057 - usage_accuracy: 1.0000 - usage_loss: 6.5498e-08 - val_loss: 0.1270 - val_type_color_style_accuracy: 0.9167 - val_type_color_style_loss: 0.1266 - val_usage_accuracy: 1.0000 - val_usage_loss: 2.7940e-09\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3s/step - loss: 0.0993 - type_color_style_accuracy: 0.9128 - type_color_style_loss: 0.0993 - usage_accuracy: 1.0000 - usage_loss: 3.5551e-08 - val_loss: 0.1265 - val_type_color_style_accuracy: 0.9074 - val_type_color_style_loss: 0.1265 - val_usage_accuracy: 1.0000 - val_usage_loss: 3.7253e-09\n"
     ]
    }
   ],
   "source": [
    "# MobileNetV2\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "usage_train = y_train[:, :4]\n",
    "type_train = y_train[:, 4:23]\n",
    "color_train = y_train[:, 23:39]\n",
    "style_train = y_train[:, 39:]\n",
    "\n",
    "usage_val = y_val[:, :4]\n",
    "type_val = y_val[:, 4:23]\n",
    "color_val = y_val[:, 23:39]\n",
    "style_val = y_val[:, 39:]\n",
    "\n",
    "usage_test = y_test[:, :4]\n",
    "type_test = y_test[:, 4:23]\n",
    "color_test = y_test[:, 23:39]\n",
    "style_test = y_test[:, 39:]\n",
    "\n",
    "y_train_type_color_style = np.concatenate([type_train, color_train, style_train], axis=1)\n",
    "\n",
    "input_layer = Input(shape=(224, 224, 3))\n",
    "x = base_model(input_layer)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "type_color_style_output = Dense(y_train_type_color_style.shape[1], activation='sigmoid', name='type_color_style')(x)\n",
    "usage_output = Dense(4, activation='softmax', name='usage')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[type_color_style_output, usage_output])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'type_color_style': 'binary_crossentropy', 'usage': 'categorical_crossentropy'},\n",
    "    metrics={'type_color_style': 'accuracy', 'usage': 'accuracy'}\n",
    ")\n",
    "\n",
    "train_steps_per_epoch = np.ceil(len(X_train) / batch_size).astype(int)\n",
    "val_steps_per_epoch = np.ceil(len(X_val) / batch_size).astype(int)\n",
    "\n",
    "train_data_gen = custom_data_gen(X_train, np.concatenate([type_train, color_train, style_train], axis=1), usage_train, batch_size)\n",
    "val_data_gen = custom_data_gen(X_val, np.concatenate([type_val, color_val, style_val], axis=1), usage_val, batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "model.save(\"multi_label_mobilenetv2_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 9s/step - loss: 0.8918 - type_color_style_accuracy: 0.4384 - type_color_style_loss: 0.5429 - usage_accuracy: 0.8100 - usage_loss: 0.3482 - val_loss: 0.2119 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.2108 - val_usage_accuracy: 1.0000 - val_usage_loss: 1.2014e-07\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5s/step - loss: 0.2286 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2287 - usage_accuracy: 1.0000 - usage_loss: 3.8822e-07 - val_loss: 0.1995 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1973 - val_usage_accuracy: 1.0000 - val_usage_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 4s/step - loss: 0.2249 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2249 - usage_accuracy: 1.0000 - usage_loss: 2.3389e-08 - val_loss: 0.1965 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1947 - val_usage_accuracy: 1.0000 - val_usage_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 4s/step - loss: 0.2160 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2160 - usage_accuracy: 1.0000 - usage_loss: 9.4868e-08 - val_loss: 0.1942 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1928 - val_usage_accuracy: 1.0000 - val_usage_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 4s/step - loss: 0.2129 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2129 - usage_accuracy: 1.0000 - usage_loss: 4.9731e-07 - val_loss: 0.1947 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1934 - val_usage_accuracy: 1.0000 - val_usage_loss: 1.1921e-07\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 4s/step - loss: 0.2092 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2092 - usage_accuracy: 1.0000 - usage_loss: 2.8458e-06 - val_loss: 0.1943 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1928 - val_usage_accuracy: 1.0000 - val_usage_loss: 1.2976e-07\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 4s/step - loss: 0.2100 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2099 - usage_accuracy: 1.0000 - usage_loss: 1.8328e-05 - val_loss: 0.1942 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1928 - val_usage_accuracy: 1.0000 - val_usage_loss: 3.5856e-07\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 4s/step - loss: 0.2083 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2083 - usage_accuracy: 1.0000 - usage_loss: 1.2649e-06 - val_loss: 0.1943 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1931 - val_usage_accuracy: 1.0000 - val_usage_loss: 7.1867e-07\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 4s/step - loss: 0.2074 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2074 - usage_accuracy: 1.0000 - usage_loss: 3.0908e-06 - val_loss: 0.1939 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1924 - val_usage_accuracy: 1.0000 - val_usage_loss: 9.7789e-07\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 4s/step - loss: 0.2083 - type_color_style_accuracy: 0.9753 - type_color_style_loss: 0.2083 - usage_accuracy: 1.0000 - usage_loss: 4.8997e-06 - val_loss: 0.1942 - val_type_color_style_accuracy: 0.9630 - val_type_color_style_loss: 0.1929 - val_usage_accuracy: 1.0000 - val_usage_loss: 1.4457e-06\n"
     ]
    }
   ],
   "source": [
    "# EfficientNetB0\n",
    "\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "usage_train = y_train[:, :4]\n",
    "type_train = y_train[:, 4:24]\n",
    "color_train = y_train[:, 24:41]\n",
    "style_train = y_train[:, 41:]\n",
    "\n",
    "usage_val = y_val[:, :4]\n",
    "type_val = y_val[:, 4:24]\n",
    "color_val = y_val[:, 24:41]\n",
    "style_val = y_val[:, 41:]\n",
    "\n",
    "usage_test = y_test[:, :4]\n",
    "type_test = y_test[:, 4:24]\n",
    "color_test = y_test[:, 24:41]\n",
    "style_test = y_test[:, 41:]\n",
    "\n",
    "input_layer = Input(shape=(224, 224, 3))\n",
    "x = base_model(input_layer)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "y_train_type_color_style = np.concatenate([type_train, color_train, style_train], axis=1)\n",
    "\n",
    "type_color_style_output = Dense(y_train_type_color_style.shape[1], activation='sigmoid', name='type_color_style')(x)\n",
    "usage_output = Dense(4, activation='softmax', name='usage')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[type_color_style_output, usage_output])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'type_color_style': 'binary_crossentropy', 'usage': 'categorical_crossentropy'},\n",
    "    metrics={'type_color_style': 'accuracy', 'usage': 'accuracy'}\n",
    ")\n",
    "\n",
    "train_steps_per_epoch = np.ceil(len(X_train) / batch_size).astype(int)\n",
    "val_steps_per_epoch = np.ceil(len(X_val) / batch_size).astype(int)\n",
    "\n",
    "train_data_gen = custom_data_gen(X_train, y_train_type_color_style, usage_train, batch_size)\n",
    "val_data_gen = custom_data_gen(X_val, y_val_type_color_style, usage_val, batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "model.save(\"multi_label_model_effnetb0.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dominant_colors(image_path, num_colors=3):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert(\"RGB\")\n",
    "    pixels = np.array(img)\n",
    "    \n",
    "    pixels = pixels.reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters=num_colors).fit(pixels)\n",
    "    \n",
    "    dominant_colors = kmeans.cluster_centers_.astype(int)\n",
    "    \n",
    "    return [tuple(color) for color in dominant_colors]\n",
    "\n",
    "def get_closest_colors(dominant_rgb_colors, color_mapping, max_matches=3):\n",
    "    matched_colors = set()\n",
    "    \n",
    "    for rgb in dominant_rgb_colors:\n",
    "        closest_color = None\n",
    "        min_distance = float('inf')\n",
    "        \n",
    "        for color_name, color_rgb in color_mapping.items():\n",
    "            distance = np.linalg.norm(np.array(rgb) - np.array(color_rgb))\n",
    "            if distance < min_distance:\n",
    "                closest_color = color_name\n",
    "                min_distance = distance\n",
    "        \n",
    "        if closest_color and closest_color not in matched_colors:\n",
    "            matched_colors.add(closest_color)\n",
    "            if len(matched_colors) >= max_matches:\n",
    "                break\n",
    "    \n",
    "    return list(matched_colors)\n",
    "\n",
    "def predict_colors(image_path, color_mapping, num_colors=3):\n",
    "    dominant_colors = extract_dominant_colors(image_path, num_colors)\n",
    "    matched_colors = get_closest_colors(dominant_colors, color_mapping)\n",
    "    return matched_colors\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def predict_single_image(model, image_path, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = {\n",
    "            \"TYPE\": 0.5,\n",
    "            \"COLOR\": 0.5,\n",
    "            \"STYLE\": 0.5,\n",
    "            \"USAGE\": 0.5\n",
    "        }\n",
    "\n",
    "    img_array = preprocess_image(image_path)\n",
    "    type_color_style_pred, usage_pred = model.predict(img_array)\n",
    "    type_color_style_binary = (type_color_style_pred > thresholds[\"TYPE\"]).astype(float)\n",
    "    usage_binary = np.argmax(usage_pred, axis=1)\n",
    "\n",
    "    return type_color_style_pred, usage_pred, type_color_style_binary, usage_binary\n",
    "\n",
    "def decode_predictions_with_probabilities(type_color_style_pred, usage_pred, thresholds):\n",
    "    decoded = {}\n",
    "\n",
    "    usage_idx = np.argmax(usage_pred, axis=1)\n",
    "    decoded[\"USAGE\"] = [(CATEGORIES[\"USAGE\"][usage_idx[0]], float(np.max(usage_pred)))]\n",
    "\n",
    "    num_type_labels = len(CATEGORIES[\"TYPE\"])\n",
    "    type_probabilities = type_color_style_pred[0, :num_type_labels]\n",
    "    type_binary = (type_probabilities > thresholds[\"TYPE\"]).astype(int)\n",
    "    decoded[\"TYPE\"] = sorted(\n",
    "        [(CATEGORIES[\"TYPE\"][i], float(type_probabilities[i])) for i, value in enumerate(type_binary) if value == 1],\n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    num_style_labels = len(CATEGORIES[\"STYLE\"])\n",
    "    style_probabilities = type_color_style_pred[0, num_type_labels:num_type_labels+num_style_labels]\n",
    "    style_binary = (style_probabilities > thresholds[\"STYLE\"]).astype(int)\n",
    "    decoded[\"STYLE\"] = sorted(\n",
    "        [(CATEGORIES[\"STYLE\"][i], float(style_probabilities[i])) for i, value in enumerate(style_binary) if value == 1],\n",
    "        key=lambda x: x[1],  \n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    num_color_labels = len(CATEGORIES[\"COLOR\"])\n",
    "    color_probabilities = type_color_style_pred[0, num_type_labels+num_style_labels:num_type_labels+num_style_labels+num_color_labels]\n",
    "    color_binary = (color_probabilities > thresholds[\"COLOR\"]).astype(int)\n",
    "    decoded[\"COLOR\"] = sorted(\n",
    "        [(CATEGORIES[\"COLOR\"][i], float(color_probabilities[i])) for i, value in enumerate(color_binary) if value == 1],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - loss: 0.2106 - type_color_style_accuracy: 0.9701 - type_color_style_loss: 0.2079 - usage_accuracy: 1.0000 - usage_loss: 7.7138e-04\n",
      "Test Loss: 0.20814372599124908\n",
      "Type-Color-Style Loss: 0.20265647768974304, Type-Color-Style Accuracy: 0.9722222089767456\n",
      "Usage Loss: 0.0005722008645534515, Usage Accuracy: 1.0\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step\n",
      "Accuracy: 0.8617283950617284\n",
      "Precision: 0.6618003489533782\n",
      "Recall: 0.7462344337972899\n",
      "F1 Score: 0.6894096165550821\n",
      "Classification Report (Type-Color-Style):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.89      0.92      4389\n",
      "         1.0       0.37      0.60      0.46       471\n",
      "\n",
      "    accuracy                           0.86      4860\n",
      "   macro avg       0.66      0.75      0.69      4860\n",
      "weighted avg       0.90      0.86      0.88      4860\n",
      "\n",
      "Usage Accuracy: 1.0\n",
      "Usage Precision: 1.0\n",
      "Usage Recall: 1.0\n",
      "Usage F1 Score: 1.0\n",
      "Classification Report (Usage):\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       curtain       0.00      0.00      0.00         0\n",
      "double purpose       0.00      0.00      0.00         0\n",
      "    upholstery       0.00      0.00      0.00         0\n",
      "  wallcovering       1.00      1.00      1.00       108\n",
      "\n",
      "      accuracy                           1.00       108\n",
      "     macro avg       0.25      0.25      0.25       108\n",
      "  weighted avg       1.00      1.00      1.00       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "test_loss, type_color_style_loss, usage_loss, type_color_style_accuracy, usage_accuracy = model.evaluate(\n",
    "    X_test, {'type_color_style': y_test_type_color_style, 'usage': usage_test}\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Type-Color-Style Loss: {type_color_style_loss}, Type-Color-Style Accuracy: {type_color_style_accuracy}\")\n",
    "print(f\"Usage Loss: {usage_loss}, Usage Accuracy: {usage_accuracy}\")\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "type_color_style_predictions = predictions[0]\n",
    "usage_predictions = predictions[1]\n",
    "\n",
    "threshold = 0.2\n",
    "binary_type_color_style_predictions = (type_color_style_predictions > threshold).astype(float)\n",
    "\n",
    "binary_type_color_style_flat = binary_type_color_style_predictions.flatten()\n",
    "usage_flat = usage_predictions.flatten()\n",
    "\n",
    "accuracy = accuracy_score(y_test_type_color_style.flatten(), binary_type_color_style_flat)\n",
    "precision = precision_score(y_test_type_color_style.flatten(), binary_type_color_style_flat, average=\"macro\")\n",
    "recall = recall_score(y_test_type_color_style.flatten(), binary_type_color_style_flat, average=\"macro\")\n",
    "f1 = f1_score(y_test_type_color_style.flatten(), binary_type_color_style_flat, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print(\"Classification Report (Type-Color-Style):\")\n",
    "print(classification_report(y_test_type_color_style.flatten(), binary_type_color_style_flat))\n",
    "\n",
    "usage_predictions_binary = np.argmax(usage_predictions, axis=1)\n",
    "usage_test_binary = np.argmax(usage_test, axis=1)\n",
    "\n",
    "usage_accuracy = accuracy_score(usage_test_binary, usage_predictions_binary)\n",
    "usage_precision = precision_score(usage_test_binary, usage_predictions_binary, average=\"macro\")\n",
    "usage_recall = recall_score(usage_test_binary, usage_predictions_binary, average=\"macro\")\n",
    "usage_f1 = f1_score(usage_test_binary, usage_predictions_binary, average=\"macro\")\n",
    "\n",
    "print(f\"Usage Accuracy: {usage_accuracy}\")\n",
    "print(f\"Usage Precision: {usage_precision}\")\n",
    "print(f\"Usage Recall: {usage_recall}\")\n",
    "print(f\"Usage F1 Score: {usage_f1}\")\n",
    "\n",
    "usage_test_indices = np.argmax(usage_test, axis=1)  \n",
    "usage_predictions_indices = np.argmax(usage_predictions, axis=1)  \n",
    "\n",
    "print(\"Classification Report (Usage):\")\n",
    "print(classification_report(\n",
    "    usage_test_indices,\n",
    "    usage_predictions_indices,\n",
    "    target_names=CATEGORIES[\"USAGE\"],\n",
    "    labels=list(range(len(CATEGORIES[\"USAGE\"]))) \n",
    "))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001ABA80A7060> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step\n",
      "Decoded Predictions: {'USAGE': [('wallcovering', 0.9999985694885254)], 'TYPE': [('wallpaper', 0.985772967338562)], 'STYLE': [('illustrative', 0.2087855488061905), ('stripes; checks; and zigzags', 0.19310256838798523), ('ethnic & oriental', 0.138838529586792), ('plain', 0.1199197992682457), ('textured', 0.10479345172643661), ('geometric', 0.1000237688422203)], 'COLOR': [('white', 0.4667302072048187), ('yellow', 0.4499638080596924), ('metallic', 0.3013284504413605), ('purple', 0.16771180927753448), ('light beige', 0.14465388655662537), ('orange', 0.11642416566610336)]}\n",
      "Matched Colors: ['Light Grey', 'Dark Grey', 'Metallic']\n"
     ]
    }
   ],
   "source": [
    "# if you dont want to train other time\n",
    "# use one of the saved models to see the output\n",
    "# model = load_model(\"multi_label_model.keras\") \n",
    "\n",
    "image_path = \"source_directory\\\\Images\\\\A20419.jpg\" # A20419 AL25303\n",
    "\n",
    "thresholds = {\n",
    "    \"TYPE\": 0.5,\n",
    "    \"COLOR\": 0.1,\n",
    "    \"STYLE\": 0.1,\n",
    "    \"USAGE\": 0.5\n",
    "}\n",
    "color_mapping = {\n",
    "    \"Black\": (0, 0, 0),          \n",
    "    \"Blue\": (0, 0, 255),         \n",
    "    \"Brown\": (139, 69, 19),      \n",
    "    \"Dark Beige\": (190, 152, 112),  \n",
    "    \"Dark Grey\": (169, 169, 169),   \n",
    "    \"Green\": (0, 255, 0),        \n",
    "    \"Light Beige\": (226, 193, 150), \n",
    "    \"Light Grey\": (211, 211, 211),  \n",
    "    \"Metallic\": (153, 153, 153), \n",
    "    \"Multicolor\": (255, 255, 255), \n",
    "    \"Orange\": (255, 165, 0),     \n",
    "    \"Pink\": (255, 192, 203),     \n",
    "    \"Purple\": (128, 0, 128),     \n",
    "    \"Red\": (255, 0, 0),         \n",
    "    \"White\": (255, 255, 255),    \n",
    "    \"Yellow\": (255, 255, 0)      \n",
    "}\n",
    "\n",
    "type_color_style_pred, usage_pred, type_color_style_binary, usage_binary = predict_single_image(model, image_path, thresholds)\n",
    "decoded_labels = decode_predictions_with_probabilities(type_color_style_pred, usage_pred, thresholds)\n",
    "predicted_colors = predict_colors(image_path, color_mapping, num_colors=3)\n",
    "\n",
    "print(\"Decoded Predictions:\", decoded_labels)\n",
    "print(\"Matched Colors:\", predicted_colors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
